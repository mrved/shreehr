---
phase: 06-ai-assistant
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - docker-compose.yml
  - prisma/schema.prisma
  - src/lib/ai/ollama-client.ts
  - src/lib/qdrant/client.ts
  - .env.example
autonomous: true
user_setup:
  - service: ollama
    why: "Self-hosted LLM runtime for AI chat"
    env_vars:
      - name: OLLAMA_BASE_URL
        source: "Local Ollama installation (default: http://localhost:11434)"
    dashboard_config:
      - task: "Install Ollama and pull model"
        location: "Terminal: curl -fsSL https://ollama.com/install.sh | sh && ollama pull llama3.2:3b"
  - service: qdrant
    why: "Vector database for policy document RAG"
    env_vars:
      - name: QDRANT_URL
        source: "Docker container (default: http://localhost:6333)"
    dashboard_config:
      - task: "Start Qdrant via docker-compose"
        location: "Terminal: docker compose up -d qdrant"

must_haves:
  truths:
    - "AI SDK packages installed and importable"
    - "Qdrant container runs via docker-compose"
    - "Conversation and Message models exist in database"
    - "PolicyDocument model exists for RAG storage"
  artifacts:
    - path: "src/lib/ai/ollama-client.ts"
      provides: "Ollama provider configuration"
      exports: ["ollama"]
    - path: "src/lib/qdrant/client.ts"
      provides: "Qdrant client singleton"
      exports: ["qdrant"]
    - path: "prisma/schema.prisma"
      provides: "Conversation, Message, PolicyDocument models"
      contains: "model Conversation"
  key_links:
    - from: "src/lib/ai/ollama-client.ts"
      to: "OLLAMA_BASE_URL"
      via: "environment variable"
      pattern: "process\\.env\\.OLLAMA_BASE_URL"
    - from: "src/lib/qdrant/client.ts"
      to: "QDRANT_URL"
      via: "environment variable"
      pattern: "process\\.env\\.QDRANT_URL"
---

<objective>
Set up AI assistant infrastructure including Vercel AI SDK, Ollama provider, Qdrant vector database, and Prisma models for conversation history and policy documents.

Purpose: Establish the foundational infrastructure for the AI chat system, enabling subsequent plans to build tools and APIs on top.
Output: Working AI SDK configuration, Qdrant container, and database models for chat persistence.
</objective>

<execution_context>
@C:\Users\ved\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ved\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-ai-assistant/06-RESEARCH.md

@prisma/schema.prisma
@package.json
@docker-compose.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install AI SDK packages and configure Ollama client</name>
  <files>
    package.json
    src/lib/ai/ollama-client.ts
    .env.example
  </files>
  <action>
1. Install required packages:
   ```bash
   pnpm add ai ollama-ai-provider @qdrant/js-client-rest
   ```

2. Create `src/lib/ai/ollama-client.ts`:
   - Import `createOllama` from `ollama-ai-provider`
   - Configure with baseURL from `process.env.OLLAMA_BASE_URL` (default: `http://localhost:11434/api`)
   - Export configured `ollama` provider for use in chat routes
   - Export embedding model helper: `ollama.embedding('nomic-embed-text')`

3. Update `.env.example` with new environment variables:
   - `OLLAMA_BASE_URL=http://localhost:11434/api`
   - `QDRANT_URL=http://localhost:6333`
   - `EMBEDDING_MODEL=nomic-embed-text` (optional, for flexibility)

Pattern to follow:
```typescript
import { createOllama } from 'ollama-ai-provider';

export const ollama = createOllama({
  baseURL: process.env.OLLAMA_BASE_URL || 'http://localhost:11434/api',
});

export const chatModel = ollama('llama3.2:3b');
export const embeddingModel = ollama.embedding('nomic-embed-text');
```
  </action>
  <verify>
    - `pnpm list ai ollama-ai-provider @qdrant/js-client-rest` shows packages installed
    - `src/lib/ai/ollama-client.ts` exists and exports `ollama`, `chatModel`, `embeddingModel`
    - TypeScript compilation passes: `pnpm tsc --noEmit`
  </verify>
  <done>
    - AI SDK packages installed (ai, ollama-ai-provider, @qdrant/js-client-rest)
    - Ollama client configured with environment variable support
    - Environment variables documented in .env.example
  </done>
</task>

<task type="auto">
  <name>Task 2: Add Qdrant to docker-compose and create client</name>
  <files>
    docker-compose.yml
    src/lib/qdrant/client.ts
  </files>
  <action>
1. Update `docker-compose.yml` to add Qdrant service:
   ```yaml
   qdrant:
     image: qdrant/qdrant:latest
     container_name: shreehr-qdrant
     ports:
       - "6333:6333"
       - "6334:6334"
     volumes:
       - qdrant_storage:/qdrant/storage
     restart: unless-stopped
   ```
   Add volume under existing volumes section:
   ```yaml
   volumes:
     postgres_data:
     redis_data:
     qdrant_storage:
   ```

2. Create `src/lib/qdrant/client.ts`:
   - Import `QdrantClient` from `@qdrant/js-client-rest`
   - Create singleton instance with URL from `process.env.QDRANT_URL`
   - Export `qdrant` client
   - Export collection name constant: `POLICIES_COLLECTION = 'hr_policies'`
   - Add `ensureCollection()` function to create collection if not exists (vector size 768 for nomic-embed-text, distance: Cosine)

Pattern to follow:
```typescript
import { QdrantClient } from '@qdrant/js-client-rest';

const QDRANT_URL = process.env.QDRANT_URL || 'http://localhost:6333';

export const qdrant = new QdrantClient({ url: QDRANT_URL });
export const POLICIES_COLLECTION = 'hr_policies';

export async function ensureCollection() {
  const collections = await qdrant.getCollections();
  const exists = collections.collections.some(c => c.name === POLICIES_COLLECTION);

  if (!exists) {
    await qdrant.createCollection(POLICIES_COLLECTION, {
      vectors: { size: 768, distance: 'Cosine' },
    });
  }
}
```
  </action>
  <verify>
    - `docker compose config` validates without errors
    - `src/lib/qdrant/client.ts` exists and exports `qdrant`, `POLICIES_COLLECTION`, `ensureCollection`
    - TypeScript compilation passes: `pnpm tsc --noEmit`
  </verify>
  <done>
    - Qdrant service added to docker-compose.yml with persistent volume
    - Qdrant client singleton created with collection management
    - Collection uses correct vector size (768) for nomic-embed-text
  </done>
</task>

<task type="auto">
  <name>Task 3: Add Prisma models for conversation history and policy documents</name>
  <files>
    prisma/schema.prisma
  </files>
  <action>
1. Add `Conversation` model to `prisma/schema.prisma`:
   ```prisma
   model Conversation {
     id        String    @id @default(cuid())
     user_id   String
     user      User      @relation(fields: [user_id], references: [id], onDelete: Cascade)
     title     String?   // Auto-generated from first message

     created_at DateTime @default(now())
     updated_at DateTime @updatedAt

     messages Message[]

     @@index([user_id, updated_at])
     @@map("conversations")
   }
   ```

2. Add `Message` model:
   ```prisma
   model Message {
     id              String       @id @default(cuid())
     conversation_id String
     conversation    Conversation @relation(fields: [conversation_id], references: [id], onDelete: Cascade)

     role    String // 'user' | 'assistant' | 'system' | 'tool'
     content String @db.Text

     // Tool call tracking (for audit and debugging)
     tool_calls Json? // Array of { name, args, result }

     created_at DateTime @default(now())

     @@index([conversation_id, created_at])
     @@map("messages")
   }
   ```

3. Add `PolicyDocument` model for RAG source tracking:
   ```prisma
   model PolicyDocument {
     id          String @id @default(cuid())
     title       String
     description String?
     category    String // 'LEAVE', 'PAYROLL', 'ATTENDANCE', 'EXPENSE', 'GENERAL'

     // Document content
     content      String @db.Text
     file_path    String? // Optional: if uploaded as file

     // Embedding status
     embedding_status EmbeddingStatus @default(PENDING)
     chunk_count      Int             @default(0)
     last_embedded_at DateTime?

     // Visibility (for RBAC filtering in RAG)
     visible_to_roles String[] // ['EMPLOYEE', 'MANAGER', 'ADMIN'] - empty = all

     is_active Boolean @default(true)

     // Audit fields
     created_at DateTime @default(now())
     created_by String?
     updated_at DateTime @updatedAt
     updated_by String?

     @@index([category, is_active])
     @@index([embedding_status])
     @@map("policy_documents")
   }

   enum EmbeddingStatus {
     PENDING
     PROCESSING
     COMPLETED
     FAILED
   }
   ```

4. Add `conversations` relation to User model:
   ```prisma
   // In User model, add:
   conversations Conversation[]
   ```

5. Run `pnpm db:push` to sync schema (development only, will prompt for migration in production)
  </action>
  <verify>
    - `pnpm db:push` completes without errors
    - `pnpm tsc --noEmit` passes
    - Database has conversations, messages, and policy_documents tables
  </verify>
  <done>
    - Conversation model stores chat sessions per user
    - Message model stores chat messages with role and tool call tracking
    - PolicyDocument model tracks RAG source documents with embedding status
    - Models support RBAC via visible_to_roles field
  </done>
</task>

</tasks>

<verification>
1. All packages installed: `pnpm list ai ollama-ai-provider @qdrant/js-client-rest`
2. Docker services defined: `docker compose config | grep -A5 qdrant`
3. TypeScript compiles: `pnpm tsc --noEmit`
4. Database schema updated: `pnpm db:push` succeeds
5. Client files exist and export correctly
</verification>

<success_criteria>
- AI SDK, Ollama provider, and Qdrant client packages installed
- Qdrant service added to docker-compose.yml with persistent storage
- Ollama client configured with environment variable support
- Qdrant client with collection management helper
- Conversation, Message, and PolicyDocument Prisma models created
- All TypeScript compilation passes
</success_criteria>

<output>
After completion, create `.planning/phases/06-ai-assistant/06-01-SUMMARY.md`
</output>
